---
title: "Optimization"
author: "Wenxuan Zhang"
date: '2023-03-20'
output: html_document
---

```{r setup, include=FALSE,warning= FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Linear Programming
Object function & saturation curve are both linear

#### Objective function 
* Maximize total profit 
* Product A : 20, Product B : 25

### Resource Constraint  
* Product A  20 resource and Product B need 12
* Only 1800 resource available per day 

### Time Constraint 
* Both product require a production time of 1/15 hour
* A working day has a total of 8 hours



```{r, warning=FALSE}

library(lpSolve)
object.in <- c(25,20)
const.mat <- matrix(c(20,12,1/15,1/15),nrow = 2, byrow = TRUE)
const.rhs <- c(1800,8)
const.dir <- c("<=","<=")
optimum <- lp(direction = "max", object.in,
              const.mat,const.dir,const.rhs)

optimum$solution
optimum$objval
```

### Quadratic Programming
Object function is quadratic and constraint are linear

#### One Dimensional Non-Linear Programming 

Golden Section Search can be used to solve one-dimensional non-linear
problems

##### Basic Steps:
* Golden Ratio Defined as 0.618
* Pick an Interval A,b containing the optimum 
* Evaludate f(x1) at x1 = a + (0.382)*(b-a) vs
f(x2) at x2 = a + 0.618*(b-a)
* if f(x1) < f(x2), continue the search in the interval[a,x1] else [x2, b]

###### example 

```{r,warning = FALSE}

f <- function(x)(print(x)-1/3)^2
xmin <- optimize(f, interval = c(0,1),tol =0.001)

#example 2: non-differentiable
f <- function(x) return(abs(x-2)+2*abs(x=1))
xmin <- optimize(f, interval = c(0,3),tol =0.001)

plot(f,0,3)

```

#### Non-Linear Multi-Dimensional Programming

```{r,warning=FALSE}
library(optimx)
#optimx(par,fn, gr = Null, Hess= Null, lower = inf,
#       upper = inf, method = '', itnmax = Null,...)

```

Multiple optimization algorithm possible:
* Gradient based
* Hessian based 
* Non-gradient based: golden section search, nelder-mead
* if constraint was provided, "L-BFGS-B" is used

Himmelblau's function 
f(x,y) = (x2 + y - 11)^2 + (x+y^2-7)^2


```{r,warning=FALSE}
# Himmelblau's function 
fn <- function(para){
 matrix.A <- matrix(para, ncol = 2)
 x <- matrix.A[,1]
 y <- matrix.A[,2]
 f.x <- (x^2 + y -11)^2 + (x+y^2-7)^2
 return(f.x) 
}

xy <- as.matrix(expand.grid(seq(-5,5,length = 101),
                            seq(-5,5, length = 101)))
colnames(xy) <- c("x","y")
df <- data.frame(fnxy = fn(xy),xy)

library(lattice)
wireframe(fnxy ~ x*y, data = df, shade = TRUE,
          drape = FALSE, scale = list(arrows=FALSE),
          screen = list(z = -240, x = -70, y =0))

```

#### Gradient-Free Method: Nelder-Mead

#### Gradient-Free Method: Gradient Descent 

#### Gradient-Free Method: Newton- Raphson

#### Gradient-Free Method: BFGS and L-BFGS-B

* Gradient information comes from an approximation of Hessian Matrix
No guaranteed conversion, especially problematic if tayler expansion does not fit well.



```{r,warning=FALSE}
library(optimx)
fn <- function(x){
  return(log2(x+1))
}
par <- c(1)
optimx(par,fn, method  = 'Nelder-Mead')
optimx(par, fn, method = 'CG')
optimx(par,fn, method = 'BFGS')
optimx(par,fn,method=c("Nelder-Mead","CG","BFGS","spg","nlm"))
#-1 should be the optimal value to minimize this target

```


### R Optimization Infrastructure

```{r,warning=FALSE}

#ROI_available_solvers()

#install.packages('ROI.plugin.glpk')
#install.packages('ROI.plugin.quadprog')
#install.packages('ROI.plugin.symphony')
library(ROI)
library(ROI.plugin.glpk)


library(ROI.plugin.glpk)
lp <- OP(objective = c(2,4,4),
         L_constraint( 
           L = matrix(c(3,2,1,4,1,3,2,2,2),nrow =3),
                      dir = c("<=","<=","<="),
                      rhs = c(60,40,80))
           ,maximum = TRUE)

sol <- ROI_solve(lp,solver = 'glpk')

#Solve quadratic problems with linear constraints

library(ROI.plugin.quadprog)
qp <- OP(Q_objective(Q = diag(1,3),
                     L = c(0,-5,0)),
         L_constraint(
           L = matrix(c(-4,-3,0,2,1,0,0,-2,1),ncol = 3,
                      byrow = TRUE),
           dir = rep (">=",3),
           rhs = c(-8,2,0)
         )
         )

sol <- ROI_solve(qp, solver = 'quadprog')

```
### Application in Stats 

LASSO regression 

```{r,warning=FALSE}

n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- 1 + x1 + x2 + rnorm(n)
X <- cbind (rep(1,n),x1,x2)
r <- lm(y~ x1 + x2)
library(quadprog)
s <- solve.QP(t(X) %*% X, t(y)%*%X, matrix(nr = 3, nc = 0),numeric(),0)
coef(r)
s$solution

# with constraints => yes, you could set constriant >=
s1 <- solve.QP(t(X) %*% X, t(y)%*%X, cbind(diag(1,3)),c(0.5,1.5,2),0)
s1$solution

s2 <- solve.QP(t(X) %*% X, t(y)%*%X, cbind(diag(1,3),-diag(1,3)),c(0.5,0.5,0.5,-0.8,-1.1,-1.2),0)
s2$solution

```
### Optimization inside Quantile Regression

```{r,warning=FALSE}
n <- 101
x <- rlnorm(n)
library(lpSolve)

A1 <- cbind(diag(2*n),0)
A2 <- cbind(diag(n),-diag(n),1)

r <- lp('min',
        c(rep(1, 2*n),0),
        rbind(A1,A2),
        c(rep (">=",2*n),rep("=",n)),
        c(rep(0, 2*n),x))
tail(r$solution,1)
median(x)

```
Introducing tau = .3 allows to calculate quantile regression

```{r,warning=FALSE}
require(lpSolve)
tau <- .3
n <- 100
x1 <-rnorm(n)
x2 <- rnorm(n)
y <- 1 + x1 + x2 + rnorm(n)
X <- cbind(rep(1,n),x1,x2)
A1 <- cbind(diag(2*n),0,0,0)
A2 <- cbind(diag(n),-diag(n),X)

r <- lp("min",c(rep(tau,n),
                rep(1-tau,n),0,0,0),
        rbind(A1,A2),
        c(rep(">=",2*n),rep ("=",n)),
        c(rep(0,2*n),y)
    )

tail(r$solution,3)
library(quantreg)
rq(y~x1+x2, tau = 0.5)



```

https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf

https://web.stanford.edu/~jurafsky/slp3/5.pdf

https://rpubs.com/dnuttle/ml-logistic-cost-func_derivative

https://www.baeldung.com/cs/gradient-descent-logistic-regression




How inscart deliver in time?
https://www.youtube.com/watch?v=JvIzB3hULCo

https://www.youtube.com/watch?v=Gtz8ca_4hVg



### Logistic Regression

Logistic regression is the baseline model for classification problems. It is one of the generalized linear model. Instead of trying to get the relationship between Y and X, generalized linear models explore the relationship between Y and f(X). 

The f(x) of logistic regression is the sigmod funtion.$$Y = 1/(1+ e(-\beta X))$$. And the model **form** is
if Y = 0
$$P(Y|X) = 1/(1+e(-\beta X)$$
if Y = 0 
$$ P(Y|X) = 1- 1/(1+e(-beta X)) $$
Thus the formula can be written as 

$$ P(Y|X) = (1/1+e(-\beta X))^{Y} + (1- 1/(1+e(-\beta X))^{(1-Y)}) $$

When I first learned the logistic regression, I was wondering, why do we choose the sigmoid function as the link
funtion? Here are a couple reason.

1. Sigmoid function could transform value between (-Inf, +Inf) to [0,,1],which is a perfect fit for probability.
2. Sigmoif function got some very good math property:
* $$1 - f(x) = f(-x)$$
* $$d(f(x)) = f(x)*(1-f(x))*x$$

These property made it easier for future calculation.

The loss function for linear regression **(y-xb)^2**. For logistic regression, we use log likelihood, which
equals $\sum Xi*P(Xi)$

$$y(log(1/(1+e(-\beta X)))) + (1-y)*(1-1/(1+e(-\beta X)))$$
To get a solution for $\beta$, we use the **gradient descent** method.  The gradient of loss function is
$$ y/f(x) - (1-y)/(1-f(x)* d(fx)$$
$$\frac{y}{f(x)}-\frac{(1-y)}{(1-f(x))*f(x)}(1-f(x))*x $$
$$ \frac{(y-f(x))}{f(x)*(1-f(x))}*f(x)(1-f(x))*x$$
$$(y-f(x))*x$$
##### Logistic Regression

```{r}
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
# admit 1 then admitted 0 then do not
# gre score
# gpa
# rank (1,2,3,4) 4 different tier 
head(mydata)

# can we use linear regression
lmodel <- lm(admit~.,data = mydata)
summary(lmodel)
# intercept not meaningful, higher the rank harder to admit, higher gre & gpa will better to admit
# R^2 0.09

xtabs(~admit + rank, data = mydata)

mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
confint(mylogit)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
exp(coef(mylogit))

pred <- ifelse(predict(mylogit,data = mydata,type = "response")>0.34,1,0)

eval_data <- data.frame(pred = pred, actual= mydata$admit)
eval_metric <- xtabs(~actual+pred, data = eval_data)
accuracy <- sum(eval_metric[col(eval_metric)==row(eval_metric)])/sum(eval_metric)
print(paste(round(accuracy,3)*100,'% of prediction is right',sep =""))

cut <- seq(0,1,length=100)

getacc<-function(cut,mydata){
  met <- c()
  for (i in (1:length(cut))){
    pred <- ifelse(predict(mylogit,data = mydata,type = "response")>cut[i],1,0)
    eval_data <- data.frame(pred = pred, actual= mydata$admit)
    eval_metric <- xtabs(~actual+pred, data = eval_data)
    accuracy <- sum(eval_metric[col(eval_metric)==row(eval_metric)])/sum(eval_metric)
    met <- c(met,accuracy)
  }
  return(met)
}

a<-getacc(cut,mydata)
plot(cut,a)

#70% of chance it is right


```

```{r,warning = FALSE}

# understand coef and make a prediction

coef_logit <- coef(mylogit)
X <- cbind(rep(1,dim(mydata)[1]),mydata[,c('gre','gpa','rank')])

link<- rowSums(X[1,] * coef_logit)
response <- 1/(1+exp(-link))
print(response)

predict(mylogit,mydata[1,],type = 'response')
predict(mylogit,mydata[1,],type = 'link')

#estimate coef by gradient 
# start 
sigmoid <- function(z)
{
g <- 1/(1+exp(-z))
return(g)
}

X <- as.matrix(mydata[,c(2,3,4)])
#Add ones to X
X <- cbind(rep(1,nrow(X)),X)
#Response variable
Y <- as.matrix(mydata$admit)

cost <- function(theta)
{
m <- nrow(X)
g <- sigmoid(X%*%theta)
J <- (1/m)*sum((-Y*log(g)) - ((1-Y)*log(1-g)))
return(J)
}

initial_theta <- rep(0,ncol(X))
#Cost at inital theta
cost(initial_theta)

theta_optim <- optim(par=initial_theta,fn=cost)
#set theta
theta <- theta_optim$par
#cost at optimal value of the theta
theta_optim$value

print(theta)
print(coef_logit)
prob <- sigmoid(as.matrix(cbind(1,mydata[1,c(2,3,4)]))%*%theta)
print(prob)
predict(mylogit,data = mydata[c(1),],type = 'response')[1]

```
Gradient Descent Implementation 
https://www.r-bloggers.com/2013/12/logistic-regression-with-r-step-by-step-implementation-part-2/
